{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "556bce9e-f718-401f-a3f2-3f9cf6fad8ca",
   "metadata": {},
   "source": [
    "# Logistic Regression From Scratch \n",
    "--- \n",
    "\n",
    "This is a full on implementation of the classification algorithm **Logistic Regression** from scratch using only **Numpy**, including :\n",
    "- Logistic Function\n",
    "- Gradient Descent\n",
    "  - Full Batch\n",
    "  - Mini-Batch\n",
    "  - Stocastic\n",
    "- Cross entropy Loss (Cost function)\n",
    "- Prediction and accuracy\n",
    "- Ridge Logistic Regression **L2**\n",
    "- Lasso Logistic Regression **L1**\n",
    "- Cross Validation\n",
    "\n",
    "There will be another jupyter notebook for the **Heart Disease** data set (mini study) + **sklearn** benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d196567-b405-453c-b82f-3276a12bf719",
   "metadata": {},
   "source": [
    "## Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5406c920-c19d-4d9e-865c-cad50fab47ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0278fa-d060-4eda-97d0-af8eec603aee",
   "metadata": {},
   "source": [
    "- We only need **numPy** to implement every logistic regression functionality in this project\n",
    "- The class implementation based can be found in the `Logistic_Regression_Scratch.py` File which contain better code practices similar to `sklearn` library "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de08a418-a655-4c79-b7ae-c8f9b6a7f9d1",
   "metadata": {},
   "source": [
    "### Generating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8de0333-6c6c-481f-8d66-2e0684da28a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dummy_data(n=1000,p=3,seed=12):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    X= np.random.randn(n,p)\n",
    "\n",
    "    coeff_true = np.array([2]*p)\n",
    "    intercept = 3\n",
    "\n",
    "    P_x = np.exp(X @ coeff_true + intercept)/(1+ np.exp(X@coeff_true + intercept))\n",
    "    \n",
    "    y = np.random.binomial(1,P_x)\n",
    "\n",
    "    return X,coeff_true, intercept , y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6060af39-8c97-4579-ba39-a9e31ad1fca8",
   "metadata": {},
   "source": [
    "- This function serves as a way to generate dummy data to set our function\n",
    "- `n` number of **observations** and `p` number of **features** or predictors\n",
    "- Binary Logistic Regression follow's a **Binomial** distribution so the **true responses** are randomly drawn from it with a probability $P(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8875e661-f439-42a9-b7b0-ac9e1775d8e5",
   "metadata": {},
   "source": [
    "$$  p(X)=\\frac{e^{\\beta_{0}+X\\beta}}{1+e^{\\beta_{0}+X\\beta}}=\\frac{1}{e^{-(\\beta+X\\beta)}+1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a976987b-0b4d-4fa5-92d9-8336415a5a49",
   "metadata": {},
   "source": [
    "- This is called the **Logistic Function** which gives us results between $0$ and $1$ also called the sigmoid function\n",
    "- $P(x)$ gives us probability results for each observation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5b656a-2f60-4347-8a70-2134c260d09b",
   "metadata": {},
   "source": [
    "### Logistic Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4858dcd1-6743-4efa-9626-dc40064241e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_function(X,intercept,beta):\n",
    "    logit = X @ beta + intercept\n",
    "\n",
    "    P_x = 1/(np.exp(-logit)+1)\n",
    "\n",
    "    return P_x , logit "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d6b095-0bd4-47ef-a0f5-1da0eae35de9",
   "metadata": {},
   "source": [
    "- This `sigmoid_function` calculated the probability of an observation as stated above\n",
    "- The `logit` is just log of the **odds**\n",
    "$$ odds =p(X)/(1-p(X))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2492e62-735d-4553-a51b-2cf16018c617",
   "metadata": {},
   "source": [
    "- The`logit` is just a **Linear Regression** equation which allow us to do **inference** and statistical analysis on **Logistic Regression** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3ed4e6-c818-41e9-925a-a26986664320",
   "metadata": {},
   "source": [
    "### Cross Entropy Loss (Cost Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eecc8381-8d4c-41b4-bd6a-00880e555eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y,X,beta,intercept):\n",
    "    \n",
    "    sigmoid_fn = sigmoid_function(X,intercept,beta)\n",
    "    const_fn = -np.mean(y.T@np.log(sigmoid_fn)+(1-y).T@np.log(1-sigmoid_fn))\n",
    "    \n",
    "    return const_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04d9c7f-89ab-4228-80ed-99620b1e3111",
   "metadata": {},
   "source": [
    "- The cost function for the **Logistic Regression** is called **cross_entropy_loss** given by : $$\\mathcal{l}(\\beta)=-[y^T \\log(p(X))+(1-y)^T\\log(1-p(X))] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a36344-00fa-4579-a29b-b110582b13b1",
   "metadata": {},
   "source": [
    "- it's simply the log likelihood of the **maximum likelihood** function\n",
    "- The **ML** is similar to the binomial distribution **PMF**\n",
    "- The `-` on the equation is simply for optimizaiton purpose to apply the **Gradient Descent** <br>\n",
    "(more information and details on the documentation pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710ddf78-3f24-4474-8306-926df926de1a",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8d4df9-701f-4c43-b837-927f01fdfcd3",
   "metadata": {},
   "source": [
    "- Time to fit our logistic regression and estimate the coefficients $beta$, This function will apply all 3 types of known gradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23408ef0-062a-43d5-b032-3b95ca784667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(lr,n_itr,batch_size,Y,X,n):\n",
    "    p = X.shape[1]\n",
    "    beta_est = np.zeros((p,1))\n",
    "    intercept_est = 0\n",
    "\n",
    "    for i in range(n_itr):\n",
    "        idx = np.random.choice(n,size = batch_size , replace = False)\n",
    "        X_GD = X[idx]\n",
    "        Y_GD = Y[idx].reshape(-1,1)\n",
    "\n",
    "        sigmoid_fn = sigmoid_function(X_GD,intercept_est,beta_est)\n",
    "\n",
    "        gradient_cel = (1/batch_size)*(X_GD.T@(sigmoid_fn-Y_GD))\n",
    "\n",
    "        gradient_intercept = (1/batch_size)*np.sum(sigmoid_fn-Y_GD)\n",
    "\n",
    "        beta_est = beta_est - (lr*gradient_cel)\n",
    "        intercept_est = intercept_est - (lr*gradient_intercept)\n",
    "\n",
    "    return beta_est , intercept_est"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb8639f-a987-43d9-a8e8-7bd715be9e66",
   "metadata": {},
   "source": [
    "- The **Logistic Regression** has no closed form solution unlike the **Linear Regression** OLS, so gradient descent is the only way to estimate the coefficients of the model, the gradient of the cost function (cross entropy loss) is : $$ \\nabla J(\\beta)=\\frac{1}{n}X^T(\\sigma(X\\beta)-y)$$\n",
    "\n",
    "And for the intercept it's : $$ \\nabla J(\\beta_{0})=\\frac{1}{n}\\sum(\\sigma(X\\beta)-y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9b6474-47d6-4225-9265-1bbeedd4573b",
   "metadata": {},
   "source": [
    "- They are simply the pratial derivaiton with respect to $\\beta$ and for the intercept for $\\beta_{0}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee85ef8-06ff-4c04-9198-8a2a7cd816a7",
   "metadata": {},
   "source": [
    "- The `idx` is simply randomly samples take batches from the data `n`\n",
    "- Both of `X_GD` and `Y_GD` are samples to used to calculate the gradient for the next step in the **Gradient Descent** algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56b74a3-20e8-4362-82ec-1cd41c209e9f",
   "metadata": {},
   "source": [
    "### Prediction & Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5a854ce-4e23-4687-a7c6-4d4c1f52c67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,intercept,beta,threshold=0.5):\n",
    "    predicted_probability = sigmoid_function(X,intercept,beta)\n",
    "\n",
    "    predicted_class = (predicted_probability>= threshold).astype(int)\n",
    "    return predicted_class, predicted_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c66d1e-5176-4165-a8e4-a0ba95a0213b",
   "metadata": {},
   "source": [
    "- This function simply calculate the probability of each observation using the estimated coefficients from the `gradient_descent` function\n",
    "- Classify based on a `threshold` usually set to $0.5$ to either $0$ or $1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f1cc5c-23a6-4044-a40e-41bf5d387d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(Y,Y_pred):\n",
    "    return np.mean(Y_pred==Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f52e53-8438-4466-83ec-0f7e6a6e92c6",
   "metadata": {},
   "source": [
    "- comparing the true values of the response `Y` and the predicted values of `Y_pred`\n",
    "- This will come in handy when we compare different regularizations and hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba8fa77-8433-43d0-95bc-1f7defa27dcd",
   "metadata": {},
   "source": [
    "### Ridge Logistic Regression "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
